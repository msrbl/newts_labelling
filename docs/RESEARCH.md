# Автоматическая разметка поз тритонов: DeepLabCut vs. YOLO и альтернативы

## 1. DeepLabCut vs. YOLO – что выбрать для ключевых точек?

**DeepLabCut (DLC)** – это специализированный инструмент для безмаркерного определения поз животных ([deeplabcut - PyPI](https://pypi.org/project/deeplabcut/#:~:text=deeplabcut%20,file%2C%20see%20%C2%B7%20Our)). Он особо эффективен на небольших датасетах, так как использует **transfer learning** (предобученные глубокие сети): даже десятки или сотни размеченных изображений могут дать высокую точность ([DeepLabCut transfer learning question : r/computervision - Reddit](https://www.reddit.com/r/computervision/comments/14z4a8e/deeplabcut_transfer_learning_question/#:~:text=Reddit%20www,parts%20to%20get%20high%20accuracy)). DLC предназначен именно для задач детекции ключевых точек; под капотом обычно сверточная сеть (ResNet, EfficientNet и др.) с деконволюционными слоями для прогнозирования координат точек. По умолчанию DLC загружает предобученные веса (например, с ImageNet) автоматически перед обучением ([DeepLabCut User Guide (for single animal projects) — DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#:~:text=By%20default%2C%20the%20pretrained%20networks,downloaded%20automatically%20before%20you%20train)), что сильно помогает при скудных данных. 

**Ultralytics YOLO (например, YOLOv8/v11 с задачей Pose)** – это универсальная модель, изначально разработанная для объектного детектирования, но у нее есть режим *pose estimation*. В этом режиме YOLO выходит за рамки прямоугольников: модель предсказывает координаты ключевых точек для каждого обнаруженного объекта. Предобученные YOLO Pose модели натренированы на датасете COCO (люди, 17 точек скелета) ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=variety%20of%20pose%20estimation%20tasks)). То есть из коробки YOLO-«pose» ориентирован на человеческую позу (17 ключевых суставов: голова, конечности и т.д. ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=In%20the%20default%20YOLO11%20pose,to%20its%20respective%20body%20joint))). Однако фреймворк поддерживает обучение на *кастомных* позах – мы можем задать свои 21 точку тритона и переобучить модель под них. YOLO тоже применяет transfer learning: можно взять готовые веса модели (натренированные на COCO) в качестве отправной точки, и дообучить их под наш датасет.

**Отличия:** DeepLabCut разработан специально для **точного определения поз животных** и часто дает очень высокую точность даже при небольшом количестве данных, благодаря мощной предобученной сети и обширным аугментациям. DLC предоставляет удобные инструменты для разметки, улучшения данных и оценки качества (интерактивная разметка, активное обучение и пр.). YOLO же – более **универсальное и быстрое** решение. YOLO v8/11 с лёгкими версиями моделей (nano, small) рассчитан на высокую скорость и может работать в реальном времени, что полезно, если планируется потоковый или быстрый оффлайн-инференс. Однако для точности поз на уровне пикселя YOLO может уступать, так как оптимизирован под комбинированную задачу (детекция + ключевые точки) одним выходом. В контексте **нашего кейса (один тритон на однообразном фоне)**: обе модели способны решить задачу. DLC может достичь высокой точности положения 21 точки “пузика” тритона даже с ~100-170 изображениями. YOLO тоже справится, но нужно правильно настроить обучение на нестандартные ключевые точки.

**Связка YOLO + DeepLabCut:** В большинстве случаев для одного объекта нет нужды использовать их одновременно – достаточно одной подходящей модели. Теоретически, их можно комбинировать: например, YOLO сначала находит расположение тритона (bounding box), а DLC затем уточняет ключевые точки внутри этой области. Но если на изображении **и так только один тритон на понятном фоне**, дополнительный этап детекции не понадобится. Комбинацию стоит рассмотреть разве что при более сложном вводе: например, если на одном кадре несколько животных или много лишнего пространства – сначала можно применять YOLO для обнаружения каждого животного, а затем запустить DLC для каждого фрагмента изображения. Однако нужно учитывать, что YOLO уже умеет предсказывать позы напрямую, поэтому вместо связки можно сразу обучить **YOLO-Pose** модель под вашего тритона и получать на выходе сразу 21 ключевую точку.

**Вывод:** использовать можно **любой из вариантов** – DeepLabCut или YOLO (например, Ultralytics YOLOv8/v11 Pose). Для **небольшого датасета** DLC часто является естественным выбором (спроектирован именно под такие случаи). YOLO же может быть проще интегрировать в код и потенциально быстрее на этапе инференса, плюс предоставляет унифицированный подход для детекции и поз. Рекомендуется попробовать тот вариант, который вам удобнее в реализации. Ниже – подробные пошаговые руководства по каждому, а затем рассмотрим и альтернативные архитектуры.

## 2. Пошаговое руководство: DeepLabCut для разметки ключевых точек

Если вы решите использовать **DeepLabCut**, вот план действий от установки до инференса:

1. **Установка DLC.** Рекомендуется использовать Anaconda/Miniconda для настройки окружения. Установите Python (например 3.8/3.9). Создайте новое окружение и установите DeepLabCut. Например, в терминале:  
   ```bash
   conda create -n dlc-env python=3.8  
   conda activate dlc-env  
   pip install deeplabcut
   ```  
   Это установит последнюю версию DLC и все необходимые зависимости. (Альтернативно можно следовать официальным инструкциям с использованием готового YAML-файла окружения.) Убедитесь, что у вас настроены драйверы/CUDA, если планируете обучать на GPU – DLC при наличии GPU будет использовать его автоматически.

2. **Подготовка проекта и данных.** DeepLabCut работает через концепцию *проекта*. Вам нужно создать новый проект DLC, указав папку с изображениями тритонов. В Python это делается так:  
   ```python
   import deeplabcut
   deeplabcut.create_new_project("TritonPoseProject", "YourName", ["путь/к/папке/с/изображениями"], copy_videos=False)
   ```  
   Эта команда создаст структуру папок проекта и сгенерирует файл `config.yaml` с настройками (в том числе список имен ключевых точек, путь к данным и пр.). Откройте `config.yaml` в текстовом редакторе и проверьте/отредактируйте ключевые параметры:
   - **Имена ключевых точек:** По умолчанию при создании проекта вы не указали явным списком 21 точку, DLC мог сгенерировать шаблон или взять имена автоматически. Найдите поле `bodyparts:` в config.yaml и пропишите там ваши 21 точки. Например:  
     ```yaml
     bodyparts:
       - point1
       - point2
       ...
       - point21
     ```  
     Используйте осмысленные названия (если точки соответствуют анатомическим частям или каким-то ориентирам на пузике тритона). Порядок **должен совпадать** с тем, как вы размечивали их в исходном датасете. Это критично: модель будет учиться предсказывать точки именно в этом порядке.
   - **Количество ключевых точек** (для проверки): должно автоматически соответствовать числу элементов в `bodyparts`. Также установите `numframes2pick` (если планируете извлекать кадры из видео, тут не актуально, у вас уже есть изображения).
   - DLC- проект обычно ориентирован на видео: если у вас просто набор изображений, ничего страшного – их можно трактовать как отдельные кадры. Можно переместить все изображения в папку проекта `labeled-data/имя_папки` и вписать путь к ним в config.yaml (`video_sets` или `image_paths`). Если create_new_project уже скопировал (если `copy_videos=True`, но мы указали False), вы можете вручную поместить данные.

   **Импорт существующей разметки:** Если у вас уже есть COCO JSON с координатами, есть два пути:
   - **Простой путь:** заново разметить через DLC. Откройте GUI для разметки командой `deeplabcut.label_frames(config_path)`. В графическом интерфейсе вы сможете кликнуть 21 точку на каждом изображении (имена точек будут теми, что в config.yaml). Это повторная работа, но для ~100 изображений может оказаться быстрее, чем разбираться с форматами. После разметки сохраните результаты – будет создан файл с разметкой (например, `CollectedData_YOURNAME.csv` и .h5 в папке проекта).
   - **Программный путь:** конвертировать ваши аннотации COCO в формат DLC. DLC хранит разметку в CSV/пандас-таблице: каждая строка – изображение, колонки – координаты X,Y для каждой точки. Вы могли бы написать скрипт на Python, который прочитает ваш `instances.json` и заполнит CSV в нужном формате. Также существуют инструменты вроде SLEAP, способные импортировать COCO и экспортировать в DLC-формат ([GUI — SLEAP (v1.4.1)](https://sleap.ai/develop/guides/gui.html#:~:text=GUI%20%E2%80%94%20SLEAP%20%28v1,csv%20%29)), но это усложняет процесс. Если вы уверены в силах, конвертируйте программно; иначе – воспользуйтесь встроенной разметкой DLC вручную, чтобы не запутаться.

3. **Обучение модели.** После подготовки данных и config-файла, запустите процесс обучения нейросети DLC. В `config.yaml` проверьте, что указаны правильные пути к изображениям и файлам разметки (DLC обычно ожидает, что вы выполнили `deeplabcut.extract_frames` и `deeplabcut.label_frames` – тогда он знает, где искать размеченные картинки). Если вы загрузили свои данные вручную, нужно сгенерировать обучающий набор. Обычно это делается командой:  
   ```python
   deeplabcut.create_training_dataset(config_path)
   ```  
   Она подготовит из разметки данные для нейросети (разобьёт на train/test, сделает аугментации и сгенерирует файлы TFRecords или аналогичные для обучения). Далее запустите обучение:  
   ```python
   deeplabcut.train_network(config_path, shuffle=1, displayiters=10, saveiters=500, maxiters=50000)
   ```  
   Здесь:
   - `shuffle=1` – номер permutation (DLC позволяет иметь несколько перестановок/разбиений данных, обычно 1 по умолчанию).
   - `displayiters=10` – как часто выводить информацию о потерях (loss) в консоль.
   - `saveiters=500` – как часто сохранять промежуточные веса.
   - `maxiters=50000` – максимальное количество итераций (можно изменить; для небольшого датасета часто хватает 50k-100k итераций, наблюдайте по метрикам).  

   **Про предобученные веса:** Ничего специально делать не нужно – DLC сам скачает предобученную модель (ResNet-50 стандартно) перед стартом обучения ([DeepLabCut User Guide (for single animal projects) — DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#:~:text=By%20default%2C%20the%20pretrained%20networks,downloaded%20automatically%20before%20you%20train)). Вы можете в config.yaml выбрать другой бэкбон (например, `resnet101` или легкий MobileNet), но ResNet50 – хороший баланс точности и скорости. Учитывая 100–170 изображений, **transfer learning** позволит быстро достичь хорошего результата. В первые несколько тысяч итераций сеть уже заметно снизит ошибку. DLC выводит метрики ошибки (Loss) и вы можете контролировать, не начинается ли переобучение – для этого следите за ошибкой на тестовой выборке (DLC разделяет данные на train/test автоматическим образом).

4. **Оценка и инференс.** После обучения (DLC сохранит лучшую модель в папке `dlc-models/.../train/`) стоит провести оценку качества:  
   ```python
   deeplabcut.evaluate_network(config_path, plotting=True)
   ```  
   Это вычислит среднюю ошибку по тестовым изображениям и может построить графики или визуализации предсказаний vs. истины. Убедившись, что точность вас устраивает, можно применять модель на новых данных. Для **инференса на изображениях** используйте функцию DLC для анализа видео/изображений. Хотя функция называется `analyze_videos`, ее можно применять и к набору кадров (по сути, изображений):  
   ```python
   deeplabcut.analyze_videos(config_path, ["путь/к/папке/или/видео.mp4"], save_as_csv=True)
   ```  
   Если у вас отдельные изображения, есть варианты:
   - Передать путь к папке вместо видео (DLC обработает все файлы изображений в ней).
   - Либо собрать изображения в видео (например, слайд-шоу из изображений) и прогнать как видео.
   - Либо воспользоваться `deeplabcut.analyze_time_lapse` (в новых версиях может быть функция для последовательности изображений).  
   На выходе DLC сохранит координаты ключевых точек (например, CSV-файл с колонками для каждого «bodypart») или даже нарисует скелет на изображениях (если вызвать `deeplabcut.plot_trajectories` или включить `videotype` для вывода). 

   > 💡 **Совет:** Поскольку у вас **офлайн-инференс**, время не критично. Вы можете использовать даже полную ResNet модель. Но если вдруг нужно ускорить предсказание (например, обрабатывать тысячи изображений), можно воспользоваться экспортом модели. DLC (начиная с версий 2.x) позволяет экспортировать модель в формат TensorFlow / TensorRT для более быстрого инференса, или даже конвертировать в ONNX. Однако для первого прототипа это не обязательно – просто используйте `analyze_videos` в Python.

5. **Улучшение модели и расширение датасета.** После первого цикла обучения посмотрите на результаты на новых изображениях. Обратите внимание, где модель ошибается:
   - Добавьте в обучающий датасет те случаи, которые сложны для модели. **Active learning** в DLC: вы можете взять несколько изображений, где ошибка велика, вручную поправить предсказанные точки (они уже будут близки к истине, нужно только слегка подвигать) и добавить их как новые тренировочные образцы. DLC позволяет доразмечивать дополнительные кадры и дополнять тренировочный набор без разметки *с нуля* для всех.
   - Используйте аугментации. В `config.yaml` есть раздел аугментаций (например, `rotation`, `flip`, `scale` и т.д.). Убедитесь, что они включены – это повысит обобщающую способность. Для **тритонов на одинаковом фоне** полезно добавить небольшие случайные повороты, изменения яркости/контраста, чтобы сеть не переобучилась на конкретное положение. DLC по умолчанию генерирует аугментации, но вы можете настроить их интенсивность.
   - **Расширение датасета:** цель – покрыть **все вариации поз тритонов**, которые могут встретиться. Если фон и ракурс всегда один, основной фактор – разные особи и разные положения лап/туловища. Постарайтесь разметить изображения так, чтобы 21 точка была различима в разных позах. Если некоторых положений нет в данных – попробуйте их получить (снять новые фото, если возможно). 170 изображений – это не много, но для одного вида позы может хватить. Тем не менее, добавление еще хотя бы пары сотен снимков повысит надежность.
   - **Предобученные модели животных:** Интересно отметить, что существуют открытые предобученные модели DLC именно для животных. Проект **DeepLabCut Model Zoo** включает модель “SuperAnimal” – универсальные весы, обученные сразу на десятках видов животны ([SuperAnimal pretrained pose estimation models for behavioral analysis | Nature Communications](https://www.nature.com/articles/s41467-024-48792-2#:~:text=Quantification%20of%20behavior%20is%20critical,10%E2%80%93100%C3%97%20more%20data%20efficient%20than))】. Такой подход дает 10–100× экономию данных по сравнению с обучением с нуля. Вы могли бы попробовать инициализировать обучение не с ImageNet, а с этих весов (например, есть модели для ящериц/грызунов с похожим телосложением). Если найдете в зоопарке модель, близкую к тритону, это может ускорить обучение и улучшить точность. Впрочем, даже без этого ваш модель сама обучится при достаточном числе итераций.

   После доразметки новых данных запустите повторно `create_training_dataset` и `train_network` (с другим shuffle или увеличив maxiters) – это итеративный процесс. Останавливайтесь, когда ошибка на контрольных данных вас устроит и модель стабильно предсказывает правильно все 21 точку.

## 3. Пошаговое руководство: YOLO (Ultralytics) для ключевых точек

Теперь рассмотрим решение через **YOLO** (здесь подразумевается версия от Ultralytics, которая поддерживает задачи Pose Estimation). Будем использовать актуальную версию, условно называемую *YOLOv11-Pose* (терминология Ultralytics к 2025 г.). Шаги схожи – установка, подготовка данных, обучение, инференс.

1. **Установка YOLO.** Разработчики Ultralytics предоставляют удобный Python-пакет. Установить его можно командой:  
   ```bash
   pip install ultralytics
   ```  
   Это установит пакет `ultralytics`, содержащий класс `YOLO` и утилиту командной строки `yolo`. Проверьте установку, запустив `yolo --help` – должна появиться справка. Также убедитесь, что у вас установлены PyTorch и необходимая среда (pip обычно подтянет PyTorch автоматически в составе ultralytics). Для GPU-версии убедитесь, что PyTorch видит CUDA (иначе будет работать на CPU). 

2. **Подготовка данных в формат YOLO.** У вас уже есть данные в COCO-совместимом формате (JSON аннотации на 21 ключевую точку). YOLO требует свой формат разметки:
   - Аннотации хранятся в текстовых файлах для каждого изображения (с координатами bounding box + ключевых точек).
   - Описание датасета — в YAML-конфиге.  

   К счастью, Ultralytics предоставляет инструмент для конвертации: **JSON2YOLO* ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#dataset-format#:~:text=YOLO%20pose%20dataset%20format%20can,the%20JSON2YOLO%20tool%20by%20Ultralytics))】. Он умеет преобразовывать COCO-формат аннотаций в yolo-формат, включая и позы. Вам нужно выполнить скрипт, указав папку с вашими COCO JSON:  
   ```bash
   git clone https://github.com/ultralytics/JSON2YOLO.git  
   python JSON2YOLO/convert.py --json_dir путь/к/аннотациям --save_dir путь/к/выходной/папке
   ```  
   Параметр `--json_dir` укажите на папку, где лежит ваш `instances.json` (и, если разделены, train/val JSON). `--save_dir` – куда сохранить YOLO-разметку. В результате конвертации вы получите:
   - Папку `labels/` с .txt файлами. Каждый `.txt` соответствует изображению и содержит строки с разметкой объектов. Формат строки для позы: `<class_id> <x_center> <y_center> <width> <height> <x1> <y1> <v1> ... <x21> <y21> <v21>`, где `v` – видимость точки (0/1). Координаты нормализованы от 0 до 1 относительно размеров изображения.
   - YAML-файл датасета (например, `dataset.yaml`), в котором будут прописаны пути к изображениям и labels, количество классов и ключевых точек. Откройте этот YAML и убедитесь, что там указано `nc: 1` (число классов, у нас только «тритон») и `nkpt: 21` (число ключевых точек). Также должен быть список имен ключевых точек или скелет (не обязательно для обучения, но для информации). Если JSON исходный содержал раздел `categories` с описанием точек и skeleton, конвертер должен их перенести.
   
   **Разбиение на train/val:** Очень важно отделить часть данных для валидации. Если ваш исходный JSON содержит, например, все 100 изображений, стоит вручную разделить – скажем, 80 на обучение, 20 на валидацию. Можно заранее разбить JSON на два (COCO format позволяет), либо после конвертации разбить папку `images/` на `images/train/` и `images/val/` и соответственно `labels/train/`, `labels/val/`. Затем в YAML вписать пути `train:` и `val:` к соответствующим подпапкам. Это нужно для контроля переобучения и выбора лучшей модели по валидации.

3. **Обучение YOLO-позы.** Теперь, когда данные готовы, запустим обучение модели YOLO под наши ключевые точки:
   - **Выбор модели:** Ultralytics предоставляет несколько предобученных моделей для поз (разных размеров). Вы можете начать с небольшой, например `yolo11n-pose.pt` (nano) или средней `yolo11s-pose.pt` (small). Более крупные – `m`, `l`, `x` дадут чуть лучше точность, но требуют больше VRAM и могут сильнее переобучиваться на малом датасете. Nano или Small должны хватить. 
   - **Запуск обучения (CLI):** Вы можете обучать через код Python *или* прямо через командную строку. Для наглядности воспользуемся CLI. Выполните команду (подставив свой YAML и выбрав модель):  
     ```bash
     yolo pose train data=dataset.yaml model=yolo11s-pose.pt epochs=100 imgsz=640
     ```  
     Разберем флаги: `pose train` указывает задачу (pose) и режим (train). `data=dataset.yaml` – наш конфиг с путями. `model=yolo11s-pose.pt` – использовать предобученную модель YOLOv11-small-pose в качестве стартовой точк ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#dataset-format#:~:text=yolo%20pose%20train%20data%3Dcoco8,epochs%3D100%20imgsz%3D640))】. `epochs=100` – число эпох (можно начать со 100 и посмотреть, этого может быть достаточно; если видите что модель еще недообучилась, увеличьте до 300+). `imgsz=640` – размер ресайза изображения для сети (640x640 пикселей, стандарт для YOLO; вы можете поставить 512 или 736, но 640 обычно оптимально). 
     
     При запуске YOLO начнет тренировать модель, выводя на экран метрики: *loss* и mAP. Вам интересен прежде всего mAP (mean Average Precision) по ключевым точкам – он отражает качество. В логах будет **mAP^pose 50-95** – интегральный показатель точности поз ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=Model%20size%20,M%29%20FLOPs))】. Следите, чтобы на валидации этот показатель рос и стабилизировался. Ultralytics также будет сохранять модели (наилучшую по val mAP и последнюю) в папку `runs/pose/train/`.

   - **Замечание:** Если обучение не запускается из-за несовпадения размеров (модель ожидала 17 точек, а у вас 21), нужно немного подкрутить конфиг. Обычно, если в `dataset.yaml` указано `nkpt: 21`, новый Ultralytics должен автоматически подстроить последнюю голову под 21 точки. Но если этого не произошло, можно явно задать:  
     ```bash
     yolo pose train data=dataset.yaml model=yolo11s-pose.yaml pretrained=yolo11s-pose.pt epochs=100
     ```  
     Здесь мы указываем архитектуру через YAML модели (`yolo11s-pose.yaml`), а параметром `pretrained=` даем путь к .pt файлу для инициализации весов (кроме последнего слоя ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#dataset-format#:~:text=yolo%20pose%20train%20data%3Dcoco8,pose.pt%20epochs%3D100%20imgsz%3D640))】. В самом YAML модели отредактируйте строку `nkpt: 21`. Это продвинутый шаг, но может понадобиться, если по умолчанию сеть не подхватила число точек.

   - **Аугментации и параметры:** По умолчанию YOLO применяет различные аугментации (флип, масштаб, цветовые сдвиги, мозаики) – это прописано внутри. Для небольшого датасета это благо, они расширят вариативность. Но если вы видите, что аугментации слишком искажают специфичный вид тритона, можно их ослабить в hyperparameters (см. `Ultralytics/docs/hyps.yaml`). На первый цикл обычно не нужно – используйте стандарт.

4. **Инференс (применение YOLO модели).** После завершения обучения у вас будет лучший вес модели, например `best.pt`. Его можно использовать для предсказания ключевых точек на новых изображениях:
   - **Через CLI:**  
     ```bash
     yolo pose predict model=path/to/best.pt source=path/to/your/new_images/ save=True
     ```  
     Параметр `source` может быть папкой с изображениями, одиночным изображением или видео. Если указать папку, YOLO обработает все изображения в ней. `save=True` заставит сохранить результаты – появится папка `predictions/` с изображениями, на которых нарисованы скелеты (соединенные ключевые точки). Также вывод координат можно перенаправить в JSON/CSV через опции, либо парсить из объекта результатов.  
   - **Через Python:**  
     ```python
     from ultralytics import YOLO
     model = YOLO("path/to/best.pt")
     results = model("путь/к/изображению.jpg")
     for r in results:
         kpts = r.keypoints.xy  # координаты (список [x,y] для каждой точки)
         confs = r.keypoints.conf  # уверенности модели в каждой точке
     ```  
     Используя Python-API, вы можете интегрировать модель в свое приложение, постобрабатывать координаты как требуется. Например, можно вычислять углы, длины или относительные положения точек по координатам.

   Результаты YOLO – это, по сути, координаты всех 21 точек для каждого обнаруженного тритона. Если на изображении гарантированно один тритон, вы просто берете первый детект. Если вдруг будет несколько, `results` вернет список с несколькими предсказаниями (каждое со своими keypoints и score). Можно фильтровать по вероятности. Но в нашем случае, скорее всего, всегда один объект.

5. **Улучшение YOLO-модели и расширение данных.** Многие подходы тут совпадают с описанными для DLC:
   - **Добавление данных:** если модель ошибается на каких-то позах, доразметьте такие примеры и добавьте в обучающую выборку. Вы можете продолжить обучать *той же моделью*: просто запустите снова `yolo pose train` с теми же параметрами, указав `epochs` побольше. Ultralytics распознает существование `last.pt` и может продолжить обучение. Либо лучше объедините старый и новый датасеты и начните обучение заново (в начале обучения с pre-trained весами модель быстро переподстроится под увеличение данных).
   - **Регулировка гиперпараметров:** для малого датасета, возможно, стоит уменьшить `batch size` (если он слишком велик) – Ultralytics сам выбирает по умолчанию максимум, но вы можете явно указать `batch=8` например. Также следите за кривой learning rate: возможно, имеет смысл немного понизить начальный LR, чтобы обучение было более плавным на небольшом объеме данных.
   - **Early stopping:** если видите, что после N эпох валидционный mAP перестал расти или начал падать – остановите обучение, чтобы не переобучиться. Можно воспользоваться встроенным callback’ом или просто прервать и использовать сохраненную лучшую модель.
   - **Предобученные веса другой задачи:** альтернативно, вы могли бы попробовать использовать веса от модели детекции объектов (без поз) в качестве основы. Но поскольку у Ultralytics уже есть предобученные *pose*-модели на человеке, их backbone уже знает многое из визуальных признаков. Даже если тритон не похож на человека, базовые фильтры (края, текстуры) будут полезны. Практика показывает, что fine-tune модели, обученной на большом датасете (COCO), сходится быстрее, чем обучение с нуля. Вы это использовали, задав `model=yolo11s-pose.pt` при старте обучения – тем самым загрузились вес ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=,recommended%20for%20training))】.
   - **Визуализация и отладка:** смотрите на сгенерированные YOLO предсказания (сохраненные изображения с нарисованными точками). Так вы заметите, если какая-то точка систематически смещена или путается с другой. Возможно, нужно более четко определить эту точку в разметке (например, всегда ли вы консистентно отмечали “центр пятна на пузе” или могли иногда смещаться). Чистота и консистентность разметки очень важны – модель не сможет угадывать, если вы сами аннотировали непоследовательно.
   - **Скорость инференса:** YOLO, особенно небольшие модели (nano, small), очень быстрый. После отладки на CPU/GPU можно задуматься об оптимизации: Ultralytics позволяет экспортировать модель в ONNX, CoreML, TensorRT и др ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=Can%20I%20export%20a%20YOLO11,to%20other%20formats%2C%20and%20how)) ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=))】. Для оффлайн-задачи на десктопе это необязательно, но если будете внедрять в приложение, конверсия в TensorRT даст максимум FPS.

## 4. Альтернативные подходы: HRNet, OpenPose, CenterNet и др.

Если по каким-то причинам DeepLabCut или YOLO вам не подходят, есть и другие популярные архитектуры для ключевых точек. Большинство из них родом из мира **human pose estimation**, но идеи можно перенести и на животных. Кратко рассмотрим три упомянутых варианта и как их можно адаптировать:

- **High Resolution Net (HRNet):** HRNet – это архитектура, ставшая знаменитой в задачах поз человека благодаря высокой точности. Ее ключевая особенность – поддержание высокого разрешения признаков на всех стадиях сети, параллельно обрабатывая несколько масштабо ([Which Human pose estimation model should you pick to realise your ideas for a video analytics product in the year 2024 | by Pallawi | Medium](https://pallawi-ds.medium.com/which-human-pose-estimation-model-should-you-pick-to-realise-your-ideas-for-a-video-analytics-6ca754cc1f4e#:~:text=Deep%20High,2019))】. Проще говоря, HRNet одновременно извлекает и мелкие детали (пиксельное расположение точки), и общие взаимосвязи (конфигурацию позы) – это дает очень точные предсказания ключевых точек. Для вашего случая HRNet могла бы быть отличным выбором с точки зрения качества. Как ее использовать? Существует проект **MMPose** (от OpenMMLab) и другие открытые реализации HRNet для поз. Вы можете взять предобученный HRNet (на том же COCO) и **дообучить его на своих данных тритонов**. Предобучение на людях, конечно, не идеально для тритона, но базовые фильтры и архитектура помогут быстро подстроиться. Вам нужно будет подготовить датасет в формате, поддерживаемом MMPose (он кстати умеет читать COCO формат сразу). Затем написать конфигурационный файл, указав число ключевых точек = 21, загрузить веса HRNet-...w32 (например) и начать fine-tune. Это потребует больше ручной работы, чем YOLO/DLC, но даст гибкость. HRNet особенно хорош, если вам критичны **минимальные ошибки** в координатах. Учтите, что HRNet – тяжелая модель (много параметров), на 170 изображениях есть риск переобучиться. Поэтому без предобученных весов ее не стоит учить с нуля. Но с transfer learning, заморозкой части слоев на первых эпохах и постепенной разморозкой – можно добиться результата. 

- **OpenPose:** это открытая модель от CMU, одна из первых для многопользовательского определения позы. OpenPose использует подход **bottom-up**: сначала сверточная сеть (на основе VGG, в оригинале) предсказывает **heatmaps** для всех ключевых точек и **парные связи** (Part Affinity Fields) для связок между точкам ([CenterNet Pose Estimation Nets  - Wolfram Neural Net Repository](https://resources.wolframcloud.com/NeuralNetRepository/resources/ad9e7395-15c4-4423-b9b5-f3f82e31be30/#:~:text=Released%20in%202019%2C%20this%20family,the%20human%20instances%20while%20ResNet))】. Затем алгоритм группирует точки в скелеты разных особей. Для вашего случая (один объект) OpenPose сводится к предсказанию heatmap’ов 21 точки. Теоретически, ее можно переобучить на тритонах: нужно заменить конфигурацию под 21 точку и скелет (в OpenPose нужно явно задать, какие точки соединять). Однако практическая сложность – OpenPose имеет свой код на C++/CUDA (или старый на Caffe). Обучение кастомной OpenPose не тривиально для новичка. Более того, **лицензия OpenPose ограничивает коммерческое использование** (бесплатно только для академических/некоммерческих целей ([Which Human pose estimation model should you pick to realise your ideas for a video analytics product in the year 2024 | by Pallawi | Medium](https://pallawi-ds.medium.com/which-human-pose-estimation-model-should-you-pick-to-realise-your-ideas-for-a-video-analytics-6ca754cc1f4e#:~:text=Openpose%20is%20available%20for%20academic,refundable%20USD%2025%2C000%20annual%20royalty))】, что может быть минусом. Тем не менее, существуют более простые реализации (на PyTorch: например, LightOpenPose). Если очень хочется попробовать bottom-up подход – можно рассмотреть **TensorRT Pose** или **OpenPifPaf** – современные варианты этого подхода. Их плюс: они сразу могут обнаруживать нескольких животных без обучающего детектора. Минус: требуют больше данных для обучения надежного ассоциации точек. В вашем проекте, где **все кадры однотипны**, OpenPose-подход может быть избыточным. Он больше полезен, когда в кадре много экземпляров и нужно их разделять.

- **CenterNet (Objects as Points):** CenterNet – это одностадийный детектор, который представил идею **anchor-free** распознавания объектов через ключевые точки. Для поз эта идея тоже применима. Авторы CenterNet показали, что можно детектировать человека как точку центра, а ключевые точки (суставы) предсказывать относительно этого центр ([CenterNet Pose Estimation Nets  - Wolfram Neural Net Repository](https://resources.wolframcloud.com/NeuralNetRepository/resources/ad9e7395-15c4-4423-b9b5-f3f82e31be30/#:~:text=Released%20in%202019%2C%20this%20family,the%20human%20instances%20while%20ResNet))】. По сути, сеть выводит heatmap центров объектов и heatmaps ключевых точек, плюс векторы смещения от центра до каждого ключа, чтобы отнести их к правильному объекту. Такой подход объединяет детекцию и позу в одной сети, аналогично тому, как это делает YOLO, но с иной техникой. Преимущество – меньше гиперпараметров (не нужны anchor-боксы), высокая скорость и точность на уровне лучших одностадийных методо ([What is CenterNet](https://www.activeloop.ai/resources/glossary/center-net/#:~:text=CenterNet%20detects%20objects%20as%20triplets,COCO%20dataset)) ([What is CenterNet](https://www.activeloop.ai/resources/glossary/center-net/#:~:text=YOLO%20,based%20detectors%20like%20YOLO))】. Вы могли бы использовать CenterNet, взяв реализацию с GitHub (например, от автора － Xingyi Zhou). Код CenterNet поддерживает обучение на MS COCO Keypoints, его можно адаптировать под 21 точку тритона. Для этого измените конфиг категории (один класс «тритон», 21 ключевая точка, своя схема skeleton). Обучение пойдет похожим образом: сеть будет учиться выдавать центр тритона и расположение точек. CenterNet тоже выигрывает от предобучения (обычно backbone – Hourglass или ResNet). Есть готовые веса hourglass, но опять же на людях. С 170 изображениями лучше начинать с предобученного backbones. CenterNet-подход интересен тем, что совместно решает задачу детекции и позы: в будущем, если добавите фон или других животных, модель уже сможет их разделять. Однако, по сложности внедрения CenterNet сопоставим с MMPose/HRNet – нужно разбираться с чужим репозиторием, настраивать слои. Если вы уже знакомы с MMDetection, то можно глянуть и на **Detectron2** от Facebook – там есть реализованная модель Keypoint R-CNN (по сути Mask R-CNN для позы). Она менее точная чем HRNet, но проще: сначала детекция (ResNet+FPN), потом небольшой голова для keypoints (на основе heatmaps). В Detectron2 можно тоже обучить на 21 точку, указав в конфиге MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.

**Советы по разметке и предобработке (общие):** Независимо от модели, качество исходной разметки сильно влияет на успех:
- **Консистентность разметки:** Убедитесь, что все 21 ключевые точки размечены последовательно на всех изображениях. Если какая-то точка иногда скрыта или не видна – решите, как вы это отмечаете. В COCO-формате у вас есть поле видимости `v` для каждой точки. Используйте его: `v=0` если точки нет в кадре, `v=1` если точка скрыта (но положение известно приблизительно), `v=2` если точка четко видн ([instances.json](file://file-11LpTGYKN1XjjD7rk6PW4m#:~:text=,2%2C%20959%2C%201487%2C%202%2C%201179)) ([instances.json](file://file-11LpTGYKN1XjjD7rk6PW4m#:~:text=809%2C%202359%2C%202%2C%20950%2C%202353%2C,2%2C%201088%2C%202356%2C%202))】. Конвертер JSON2YOLO перенесет эти флаги. Модель тогда не будет пытаться предсказывать несуществующие точки. Если все точки всегда видимы (например, вы фотографировали тритона снизу, и ничего не перекрыто), тогда везде `v=2` и эта проблема проще.
- **Предобработка изображений:** Раз все снимки в одинаковых условиях, особой предобработки не нужно. Но **уберите лишнее** – например, обрежьте изображения так, чтобы тритон занимал большую часть кадра. Не оставляйте огромные поля фона, это только мешает обучению (модель тратит ресурсы на пустой фон). Как советуют разработчики DLC, лучше изначально кропнуть область интереса перед обучение ([DeepLabCut User Guide (for single animal projects) — DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#:~:text=CRITICAL%20POINT%3A%20It%20is%20advisable,yaml%20file))】. Можно автоматически обрезать по bounding box, раз он у вас есть в аннотациях.
- **Нормализация:** Если фон и освещение одинаковы, то модель и так легко научится. Но бывает полезно слегка менять освещение (аугментация) чтобы модель не переобучилась на конкретную яркость. В DLC это настраивается, в YOLO включено по умолчанию.
- **Скелет (связи между точками):** Определите для себя, какие пары точек логично считать связанными (для визуализации и для понимания модели). Например, если 21 точка – это контур пузика, имеет смысл соединить их последовательной линией. Skeleton-схема обычно указывается в формате аннотаций (COCO JSON у вас мог содержать `"skeleton": [...pairs...]`). Это не влияет прямо на обучение (разве что OpenPose явно учит связи), но помогает при отладке – удобнее видеть, что связи нарисованы правильно, тогда сразу заметно, если какая-то точка перепутана местами.
- **Маленькие объекты/точки:** Если точки расположены очень близко (например, образуют плотный кластер), модель может путать их. Можно немного сместить фокус: либо использовать более высокое разрешение изображения при подаче в сеть (например, 1280px, если позволяют ресурсы, так сеть получит больше пикселей на детали), либо попробовать модель с более высоким разрешением выходных heatmap (HRNet в этом плане хорош). В YOLO вы ограничены сеткой 640px – но можно увеличить `imgsz`, тогда и выходная карта будет крупнее.
- **Проверка метрик:** Для поз есть своя метрика – **PCK** (Percentage of Correct Keypoints) или mAP по ключевым точкам. Используйте их для объективной оценки. Например, mAP (OKS) > 0.9 на валидации будет означать, что большинство точек предсказываются практически на месте. Если какая-то точка упрямо не детектируется (mAP низкий только для нее), пересмотрите разметку этой точки: возможно, она плохо определена визуально или размечена непоследовательно.

Наконец, **резюмируя**: для вашего объема данных и задачи рекомендуется начать с **DeepLabCut**, так как он специально разработан для таких случаев и уже многие исследователи успешно отмечали позы животных даже на десятке примеров. Если хотите более код-ориентированное решение или интеграцию с детектором, попробуйте **YOLO-Pose** – он даст единый аккуратный pipeline. Оба подхода поддерживают использование предобученных моделей, что критично при небольшом датасете, и позволяют постепенно улучшать модель по мере добавления данных. Если же потребуется максимальная точность или более сложные сценарии (несколько тритонов, разные ракурсы), рассмотрите подключение более сложных архитектур (HRNet, CenterNet и др.) или их комбинаций (например, детектировать тритона одним моделем, а координаты уточнять другой). Но, как правило, для начала DeepLabCut или YOLO покрывают все требования.

**Источники и документация:**

- Описание возможности **YOLO** для pose estimation и предобученных моделей на COC ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=YOLO11%20pose%20models%20use%20the,variety%20of%20pose%20estimation%20tasks))】, включая использование 17 точек для человека по умолчанию.  
- Инструкция Ultralytics по **обучению кастомной модели позы** (пример кода ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#:~:text=,recommended%20for%20training))】.  
- Инструмент **JSON2YOLO** для конвертации COCO-аннотаций в формат YOL ([Pose Estimation - Ultralytics YOLO Docs](https://docs.ultralytics.com/tasks/pose/#dataset-format#:~:text=YOLO%20pose%20dataset%20format%20can,the%20JSON2YOLO%20tool%20by%20Ultralytics))】.  
- DeepLabCut – официальный репозиторий и документация (маркерное отслеживание поз животных ([deeplabcut - PyPI](https://pypi.org/project/deeplabcut/#:~:text=deeplabcut%20,file%2C%20see%20%C2%B7%20Our))】.  
- Комментарий о преимуществах DLC на маленьких датасетах благодаря transfer learnin ([DeepLabCut transfer learning question : r/computervision - Reddit](https://www.reddit.com/r/computervision/comments/14z4a8e/deeplabcut_transfer_learning_question/#:~:text=Reddit%20www,parts%20to%20get%20high%20accuracy))】.  
- Рекомендация DLC по **кадрированию изображений** для ускорения обучени ([DeepLabCut User Guide (for single animal projects) — DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#:~:text=CRITICAL%20POINT%3A%20It%20is%20advisable,yaml%20file))】.  
- Архитектура **HRNet** – высокая точность за счет многоуровневых признако ([Which Human pose estimation model should you pick to realise your ideas for a video analytics product in the year 2024 | by Pallawi | Medium](https://pallawi-ds.medium.com/which-human-pose-estimation-model-should-you-pick-to-realise-your-ideas-for-a-video-analytics-6ca754cc1f4e#:~:text=Deep%20High,2019))】.  
- Ограничения лицензии **OpenPose** для коммерческого использовани ([Which Human pose estimation model should you pick to realise your ideas for a video analytics product in the year 2024 | by Pallawi | Medium](https://pallawi-ds.medium.com/which-human-pose-estimation-model-should-you-pick-to-realise-your-ideas-for-a-video-analytics-6ca754cc1f4e#:~:text=Openpose%20is%20available%20for%20academic,refundable%20USD%2025%2C000%20annual%20royalty))】.